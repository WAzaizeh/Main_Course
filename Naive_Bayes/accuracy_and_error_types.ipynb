{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Reload our Naive Bayes Classifier from 2.2\n",
    "\n",
    "Here we'll quickly reload the Naive Bayes classifier from earlier. This is all code you've seen before. It is worth noting how little code is actually required to generate this model. It's a relatively simple exercise, and SKLearn makes it impressively easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Grab and process the raw data.\n",
    "data_path = (\"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "             \"master/sms_spam_collection/SMSSpamCollection\"\n",
    "            )\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['spam', 'message']\n",
    "\n",
    "# Enumerate our spammy keywords.\n",
    "keywords = ['click', 'offer', 'winner', 'buy', 'free', 'cash', 'urgent']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    ")\n",
    "\n",
    "sms_raw['allcaps'] = sms_raw.message.str.isupper()\n",
    "sms_raw['spam'] = (sms_raw['spam'] == 'spam')\n",
    "data = sms_raw[keywords + ['allcaps']]\n",
    "target = sms_raw['spam']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Success Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now we have our model as well as our returned predictions. \n",
    "\n",
    "The first thing to note is what data is directly comparable for model evaluation: our target and y_pred variables. Target is the actual outcomes, whether something was spam or ham. The y_pred is the predicted outcomes from our classifier. Both are ordered arrays with the results from each row of the dataframe. When the two agree that means our model was able to successfully predict whether a given message was spam or ham. When they disagree our model was incorrect.\n",
    "\n",
    "The most basic measure of success, then, is how often our model was correct. This is called the accuracy. It's a metric you've seen before as it was our method of evaluation in the past lesson, but translated from a count to a rate or percentage.\n",
    "\n",
    "Go ahead and calculate it in the cell below. If you're stuck look back at the previous lesson. If you haven't yet, make your own copy of this notebook to work with locally so you don't lose your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 89.16%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of your model here.\n",
    "accuracy = (target == y_pred).sum()/target.shape[0]\n",
    "print('Model accuracy: {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "You should be getting __89.16%__ off of 4968 correctly classified messages and 604 incorrectly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The next level of analysis of your classifier is often something called a Confusion Matrix. This is a matrix that shows the count of each possible permutation of target and prediction. So in our case, it will show the counts for when a message was ham and we predicted ham, when a message was ham and we predicted spam, when a message was spam and we predicted ham, and when a message was spam and we predicted spam.\n",
    "\n",
    "SKLearn has a built in confusion matrix function, so let's quickly import that and generate one here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4770,   55],\n",
       "       [ 549,  198]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## DRILL:\n",
    "\n",
    "It's worth calculating these with code so that you fully understand how these statistics work, so here is your task for the cell below. Manually generate (__meaning don't use the SKLearn function__) your own confusion matrix and print it along with the sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4770   55]\n",
      " [ 549  198]]\n"
     ]
    }
   ],
   "source": [
    "# Build your confusion matrix and calculate sensitivity and specificity here.\n",
    "pos_index = target.index[target] # all spam\n",
    "neg_index = target.index[~target] # all ham\n",
    "confusion_matrix = (np.zeros(shape=(2,2))).astype(int)\n",
    "confusion_matrix[0,0] = (target[neg_index] == y_pred[neg_index]).sum()\n",
    "confusion_matrix[0,1] = len(target[neg_index]) - confusion_matrix[0,0]\n",
    "confusion_matrix[1,1] = (target[pos_index] == y_pred[pos_index]).sum()\n",
    "confusion_matrix[1,0] = len(target[pos_index]) - confusion_matrix[1,1]\n",
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "96px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
