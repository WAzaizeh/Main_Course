{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Grab and process the raw data.\n",
    "data_path = (\"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "             \"master/sms_spam_collection/SMSSpamCollection\"\n",
    "            )\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['spam', 'message']\n",
    "\n",
    "# Enumerate our spammy keywords.\n",
    "keywords = ['click', 'offer', 'winner', 'buy', 'free', 'cash', 'urgent']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    ")\n",
    "\n",
    "sms_raw['allcaps'] = sms_raw.message.str.isupper()\n",
    "sms_raw['spam'] = (sms_raw['spam'] == 'spam')\n",
    "data = sms_raw[keywords + ['allcaps']]\n",
    "target = sms_raw['spam']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.884304932735426\n",
      "Testing on Sample: 0.8916008614501076\n"
     ]
    }
   ],
   "source": [
    "# Test your model with different holdout groups.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores look really consistent! It doesn't seem like our model is overfitting. Part of the reason for that is that it's so simple (more on that in a bit). But we should look and see if any other issues are lurking here. So let's try a more robust evaluation technique, cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Cross Validation\n",
    "\n",
    "Cross validation is a more robust version of holdout groups. Instead of creating just one holdout, you create several.\n",
    "\n",
    "The way it works is this: start by breaking up your data into several equally sized pieces, or __folds__. Let's say you make _x_ folds. You then go through the training and testing process _x_ times, each time with a different fold held out from the training data and used as the test set. The number of folds you create is up to you, but it will depend on how much data you want in your testing set. At its most extreme, you're creating the same number of folds as you have observations in your data set. This kind of cross validation has a special name: __Leave One Out__. Leave one out is useful if you're worried about single observations skewing your model, whereas large folds combat more general overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89784946, 0.89426523, 0.89426523, 0.890681  , 0.89605735,\n",
       "       0.89048474, 0.88150808, 0.89028777, 0.88489209, 0.89568345])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's exactly what we'd hope to see. The array that `cross_val_score` returns is a series of accuracy scores with a different hold out group each time. If our model is overfitting at a variable amount, those scores will fluctuate. Instead, ours are relatively consistent.\n",
    "\n",
    "Above we used the SKLearn built in functions for both of these kinds of cross validation, the documentation for which can be found [here](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-with-stratification-based-on-class-labels). However, the outputs from that are somewhat limited. By default it uses the `score` method. You can adjust what is returned, but you don't get all of the error types or outputs you may be interested in. That's why it's not uncommon for people to code up their own cross validation.\n",
    "\n",
    "To make sure you understand how cross validation works, try to code it up yourself below, not relying on SKLearn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87634409]\n",
      " [0.89605735]\n",
      " [0.87253142]\n",
      " [0.91202873]\n",
      " [0.90305206]\n",
      " [0.86355476]\n",
      " [0.91202873]\n",
      " [0.88689408]\n",
      " [0.90843806]\n",
      " [0.88150808]]\n"
     ]
    }
   ],
   "source": [
    "# Implement your own cross validation with your spam model.\n",
    "def custom_cross_val_score(func, data, target, cv=10):\n",
    "    shuffled_data = data.sample(frac=1)\n",
    "    split_data = np.array_split(shuffled_data, cv)\n",
    "    res = np.empty(shape=(cv,1))\n",
    "    for i in range(cv): # split test and train based on excluded group indices\n",
    "        X_train = data[~data.index.isin(split_data[i].index)]\n",
    "        X_test = data[data.index.isin(split_data[i].index)]\n",
    "        y_train = target[~target.index.isin(split_data[i].index)]\n",
    "        y_test = target[target.index.isin(split_data[i].index)]\n",
    "        res[i] = func.fit(X_train, y_train).score(X_test, y_test)\n",
    "    print(res)\n",
    "    \n",
    "custom_cross_val_score(func=bnb, data=data, target=target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## What's a good score?\n",
    "\n",
    "When we're looking at this model, we've been getting accuracy scores around .89. Intuitively that seems like a pretty good score, but in the start of this lesson we mentioned different kinds of error. We also mentioned class imbalance. Both of these things are at play here. Using the topics we introduced earlier in this lesson, try to do a more in depth evaluation of the model looking at the kind of errors we're generating and what accuracy we'd get if we just randomly guessed. You may want to use what's known as a [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to show different kinds of errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix from run number:0\n",
      " [[476.   3.]\n",
      " [ 50.  29.]] \n",
      "\n",
      "Confusion matrix from run number:1\n",
      " [[474.   7.]\n",
      " [ 54.  23.]] \n",
      "\n",
      "Confusion matrix from run number:2\n",
      " [[473.   6.]\n",
      " [ 57.  21.]] \n",
      "\n",
      "Confusion matrix from run number:3\n",
      " [[480.   7.]\n",
      " [ 53.  17.]] \n",
      "\n",
      "Confusion matrix from run number:4\n",
      " [[475.   7.]\n",
      " [ 59.  16.]] \n",
      "\n",
      "Confusion matrix from run number:5\n",
      " [[479.   4.]\n",
      " [ 61.  13.]] \n",
      "\n",
      "Confusion matrix from run number:6\n",
      " [[476.   6.]\n",
      " [ 60.  15.]] \n",
      "\n",
      "Confusion matrix from run number:7\n",
      " [[489.   5.]\n",
      " [ 42.  21.]] \n",
      "\n",
      "Confusion matrix from run number:8\n",
      " [[479.   4.]\n",
      " [ 50.  24.]] \n",
      "\n",
      "Confusion matrix from run number:9\n",
      " [[469.   6.]\n",
      " [ 63.  19.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform your additional evaluation here.\n",
    "def confussion_cross_val_score(func, data, target, cv=10):\n",
    "    shuffled_data = data.sample(frac=1)\n",
    "    split_data = np.array_split(shuffled_data, cv)\n",
    "    res = np.empty(shape=(cv*2, 2))\n",
    "    for i in range(cv): # split test and train based on excluded group indices\n",
    "        X_train = data[~data.index.isin(split_data[i].index)]\n",
    "        X_test = data[data.index.isin(split_data[i].index)]\n",
    "        y_train = target[~target.index.isin(split_data[i].index)]\n",
    "        y_test = target[target.index.isin(split_data[i].index)]\n",
    "        y_pred = func.fit(X_train, y_train).predict(X_test)\n",
    "        row =i*2\n",
    "        res[row:row+2, 0:2] = confusion_matrix(y_test, y_pred)\n",
    "        print('Confusion matrix from run number:{}\\n'.format(i), res[row:row+2, 0:2],'\\n')\n",
    "    \n",
    "confussion_cross_val_score(func=bnb, data=data, target=target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "\n",
    "## Thinking like a Data Scientist\n",
    "\n",
    "How you choose to validate your model in real life will depend upon the kind of data you're working with and the kinds of concerns you have about the model's performance. Remember, your model is trained to fit the data you feed it, so if the situation changes your model will become less accurate. For example, if there are seasonal changes to your observed variable but you only train on one month's data, you're going to have a problem. You could test that by seeing how accurate your model is with a specific time period as your holdout, rather than a random sample. We'll cover techniques for dealing with time more later.\n",
    "\n",
    "## Overfitting and Naive Bayes\n",
    "\n",
    "Overfitting is always possible, but some models are more susceptible to it than others. Naive Bayes is actually pretty good for avoiding overfitting. This is largely because the assumptions are so simple, particularly the assumed independence between any two independent variables. One of the sources of overfitting is when a model tries to map complex interactions between variables that aren't really there or significant. Naive Bayes cannot do this because it assumes they are all independent and therefore not interacting. It's a nice characteristic at times, but it does mean it doesn't take into account how your features affect each other.\n",
    "\n",
    "Also, one final note on our models here. They weren't overfitting, but they weren't telling us much either. They were just barely more accurate than the dominant class. Discuss with your mentor why that is and what you could do to improve the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
